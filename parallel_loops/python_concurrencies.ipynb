{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create many files that we can open later\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "from random import random\n",
    "\n",
    "# save data to a file\n",
    "def save_file(filepath, data):\n",
    "    # open the file\n",
    "    with open(filepath, 'w') as handle:\n",
    "        # save the data\n",
    "        handle.write(data)\n",
    "\n",
    "# generate a line of mock data of 10 random data points\n",
    "def generate_line():\n",
    "    return ','.join([str(random()) for _ in range(10)])\n",
    "\n",
    "# generate file data of 10K lines each with 10 data points\n",
    "def generate_file_data():\n",
    "    # generate many lines of data\n",
    "    lines = [generate_line() for _ in range(10000)]\n",
    "    # convert to a single ascii doc with new lines\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# generate 10K files in a directory\n",
    "def generate_all_files(path='../../tmp'):\n",
    "# create a local directory to save files\n",
    "    makedirs(path, exist_ok=True)\n",
    "    # create all files\n",
    "    for i in range(10000):\n",
    "    # generate data\n",
    "        data = generate_file_data()\n",
    "        # create filenames\n",
    "        filepath = join(path, f'data-{i:04d}.csv')\n",
    "        # save data file\n",
    "        save_file(filepath, data)\n",
    "        # report progress\n",
    "        print(f'.saved {filepath}')\n",
    "\n",
    "# entry point, generate all of the files\n",
    "generate_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files sequentially\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "# open a file and return the contents\n",
    "def load_file(filepath):\n",
    "    # open the file\n",
    "    with open(filepath, 'r') as handle:\n",
    "        # return the contents\n",
    "        return handle.read()\n",
    "\n",
    "# load all files in a directory into memory\n",
    "def load_all_files(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # load each file in the directory\n",
    "    for filepath in paths:\n",
    "        # open the file and load the data\n",
    "        data = load_file(filepath)\n",
    "        # report progress\n",
    "        print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    load_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files concurrently with threads\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "# open a file and return the contents\n",
    "def load_file(filepath):\n",
    "    # open the file\n",
    "    with open(filepath, 'r') as handle:\n",
    "        # return the contents\n",
    "        return handle.read(), filepath\n",
    "\n",
    "# load all files in a directory into memory\n",
    "def main(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # create the thread pool\n",
    "    with ThreadPoolExecutor(10) as executor:\n",
    "        # submit all tasks\n",
    "        futures = [executor.submit(load_file, p) for p in paths]\n",
    "        # process all results\n",
    "        for future in as_completed(futures):\n",
    "            # open the file and load the data\n",
    "            data, filepath = future.result()\n",
    "            # report progress\n",
    "            print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files concurrently with threads in batch\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "# open a file and return the contents\n",
    "def load_files(filepaths):\n",
    "    data_list = list()\n",
    "    # load each file\n",
    "    for filepath in filepaths:\n",
    "        # open the file\n",
    "        with open(filepath, 'r') as handle:\n",
    "            # return the contents\n",
    "            data_list.append(handle.read())\n",
    "    return data_list, filepaths\n",
    "\n",
    "# load all files in a directory into memory\n",
    "def main(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # determine chunksize\n",
    "    n_workers = 100\n",
    "    chunksize = round(len(paths) / n_workers)\n",
    "    # create the thread pool\n",
    "    with ThreadPoolExecutor(n_workers) as executor:\n",
    "        futures = list()\n",
    "        # split the load operations into chunks\n",
    "        for i in range(0, len(paths), chunksize):\n",
    "            # select a chunk of filenames\n",
    "            filepaths = paths[i:(i + chunksize)]\n",
    "            # submit the task\n",
    "            future = executor.submit(load_files, filepaths)\n",
    "            futures.append(future)\n",
    "        # process all results\n",
    "        for future in as_completed(futures):\n",
    "            # open the file and load the data\n",
    "            _, filepaths = future.result()\n",
    "            for filepath in filepaths:\n",
    "                # report progress\n",
    "                print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files concurrently with processes\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "# open a file and return the contents\n",
    "def load_file(filepath):\n",
    "    # open the file\n",
    "    with open(filepath, 'r') as handle:\n",
    "        # return the contents\n",
    "        return handle.read(), filepath\n",
    "\n",
    "# load all files in a directory into memory\n",
    "def main(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # create the process pool\n",
    "    with ProcessPoolExecutor(4) as executor:\n",
    "        # submit all tasks\n",
    "        futures = [executor.submit(load_file, p) for p in paths]\n",
    "        # process all results\n",
    "        for future in as_completed(futures):\n",
    "            # open the file and load the data\n",
    "            data, filepath = future.result()\n",
    "            # report progress\n",
    "            print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files concurrently with processes in batch\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "# open a file and return the contents\n",
    "def load_files(filepaths):\n",
    "    data_list = list()\n",
    "    # load each file\n",
    "    for filepath in filepaths:\n",
    "        # open the file\n",
    "        with open(filepath, 'r') as handle:\n",
    "            # return the contents\n",
    "            data_list.append(handle.read())\n",
    "    return data_list, filepaths\n",
    "\n",
    "# load all files in a directory into memory\n",
    "def main(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # determine chunksize\n",
    "    chunksize = 20\n",
    "    # create the process pool\n",
    "    with ProcessPoolExecutor(4) as executor:\n",
    "        futures = list()\n",
    "        # split the load operations into chunks\n",
    "        for i in range(0, len(paths), chunksize):\n",
    "            # select a chunk of filenames\n",
    "            filepaths = paths[i:(i + chunksize)]\n",
    "            # submit the task\n",
    "            future = executor.submit(load_files, filepaths)\n",
    "            futures.append(future)\n",
    "            # process all results\n",
    "        for future in as_completed(futures):\n",
    "            # open the file and load the data\n",
    "            _, filepaths = future.result()\n",
    "            for filepath in filepaths:\n",
    "                # report progress\n",
    "                print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files concurrently with processes and threads in batch\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "# load a file and return the contents\n",
    "def load_file(filepath):\n",
    "    # open the file\n",
    "    with open(filepath, 'r') as handle:\n",
    "        # return the contents\n",
    "        handle.read()\n",
    "\n",
    "# return the contents of many files\n",
    "def load_files(filepaths):\n",
    "    # create a thread pool\n",
    "    with ThreadPoolExecutor(len(filepaths)) as exe:\n",
    "        # load files\n",
    "        futures = [exe.submit(load_file, name) for name in filepaths]\n",
    "        # collect data\n",
    "        data_list = [future.result() for future in futures]\n",
    "        # return data and file paths\n",
    "        return (data_list, filepaths)\n",
    "\n",
    "# load all files in a directory into memory\n",
    "def main(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # determine chunksize\n",
    "    n_workers = 8\n",
    "    chunksize = round(len(paths) / n_workers)\n",
    "    # create the process pool\n",
    "    with ProcessPoolExecutor(n_workers) as executor:\n",
    "        futures = list()\n",
    "        # split the load operations into chunks\n",
    "        for i in range(0, len(paths), chunksize):\n",
    "            # select a chunk of filenames\n",
    "            filepaths = paths[i:(i + chunksize)]\n",
    "            # submit the task\n",
    "            future = executor.submit(load_files, filepaths)\n",
    "            futures.append(future)\n",
    "        # process all results\n",
    "        for future in as_completed(futures):\n",
    "            # open the file and load the data\n",
    "            _, filepaths = future.result()\n",
    "            for filepath in filepaths:\n",
    "                # report progress\n",
    "                print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry poimt\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files concurrently with asyncio\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "# open a file and return the contents\n",
    "async def load_file(filepath):\n",
    "    # open the file\n",
    "    async with aiofiles.open(filepath, 'r') as handle:\n",
    "        # return the contents\n",
    "        return (await handle.read(), filepath)\n",
    "\n",
    "# load all files in a directory into memory\n",
    "async def main(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # create coroutines\n",
    "    tasks = [load_file(filepath) for filepath in paths]\n",
    "    # execute tasks and process results as they are completed\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        # open the file and load the data\n",
    "        data, filepath = await task\n",
    "        # report progress\n",
    "        print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files concurrently with asyncio\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "# open a file and return the contents\n",
    "async def load_file(filepath, semaphore):\n",
    "    # acquire the semaphore\n",
    "    async with semaphore:\n",
    "        # open the file\n",
    "        async with aiofiles.open(filepath, 'r') as handle:\n",
    "            # return the contents\n",
    "            return (await handle.read(), filepath)\n",
    "\n",
    "# load all files in a directory into memory\n",
    "async def main(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # create a semaphore to limit open files\n",
    "    semaphore = asyncio.Semaphore(100)\n",
    "    # create coroutines\n",
    "    tasks = [load_file(filepath, semaphore) for filepath in paths]\n",
    "    # execute tasks and process results as they are completed\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        # open the file and load the data\n",
    "        data, filepath = await task\n",
    "        # report progress\n",
    "        print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load many files concurrently with asyncio in batch\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "# load and return the contents of a list of file paths\n",
    "async def load_files(filepaths):\n",
    "    # load all files\n",
    "    data_list = list()\n",
    "    for filepath in filepaths:\n",
    "        # open the file\n",
    "        async with aiofiles.open(filepath, 'r') as handle:\n",
    "            # load the contents and add to list\n",
    "            data = await handle.read()\n",
    "            # store loaded data\n",
    "            data_list.append(data)\n",
    "    return (data_list, filepaths)\n",
    "\n",
    "# load all files in a directory into memory\n",
    "async def main(path='../../tmp'):\n",
    "    # prepare all of the paths\n",
    "    paths = [join(path, filepath) for filepath in listdir(path)]\n",
    "    # split up the data\n",
    "    chunksize = 10\n",
    "    # split the operations into chunks\n",
    "    tasks = list()\n",
    "    for i in range(0, len(paths), chunksize):\n",
    "        # select a chunk of filenames\n",
    "        filepaths = paths[i:(i + chunksize)]\n",
    "        # define the task\n",
    "        tasks.append(load_file(filepaths))\n",
    "        # execute tasks and process results as they are completed\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        # wait for the next task to complete\n",
    "        _, filepaths = await task\n",
    "        # process results\n",
    "        for filepath in filepaths:\n",
    "            # report progress\n",
    "            print(f'.loaded {filepath}')\n",
    "    print('Done')\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../../tmp'\n",
    "paths = listdir(path)\n",
    "# determine chunksize\n",
    "num_workers = 100\n",
    "# create the thread pool\n",
    "with ThreadPoolExecutor(num_workers) as executor:\n",
    "    futures = list()\n",
    "    # split the load operations into chunks\n",
    "    for i in range(num_workers):\n",
    "        # select a chunk of filenames\n",
    "        filepaths = [paths[i::num_workers] for i in range(num_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../../tmp'\n",
    "paths = os.listdir(path)\n",
    "# determine chunksize\n",
    "num_chunks = 30\n",
    "# create the thread pool\n",
    "filepaths = [paths[i::num_chunks] for i in range(num_chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../../tmp'\n",
    "paths = os.listdir(path)\n",
    "# determine chunksize\n",
    "chunk_size = 3000\n",
    "# create the thread pool\n",
    "filepaths = [paths[i:i+chunk_size] for i in range(0, len(paths), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n",
      "3000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for l in filepaths:\n",
    "    print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_pairs = [[filepaths[i], filepaths[j]] for i in range(len(filepaths)) for j in range(i, len(filepaths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for l in chunk_pairs:\n",
    "    print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = list(range(220_000))\n",
    "chunk_size = 40_000\n",
    "chunks = [pmids[i:i+chunk_size] for i in range(0, len(pmids), chunk_size)]\n",
    "chunk_pairs = [[chunks[i], chunks[j]] for i in range(len(chunks)) for j in range(i, len(chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24199890000\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in range(len(pmids)):\n",
    "    for j in range(i+1, len(pmids)):\n",
    "        pair = [pmids[i], pmids[j]]\n",
    "        n += 1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 24199890000\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "def make_pairs(chunk_pair):\n",
    "    n = 0\n",
    "    for i in chunk_pair[0]:\n",
    "        for j in chunk_pair[1]:\n",
    "            if i < j:\n",
    "                pair = [i, j]\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "# load all files in a directory into memory\n",
    "def main(chunk_pairs):\n",
    "    total_pairs = 0\n",
    "    # create the thread pool\n",
    "    with ThreadPoolExecutor(len(chunk_pairs)) as executor:\n",
    "        # submit all tasks\n",
    "        futures = [executor.submit(make_pairs, p) for p in chunk_pairs]\n",
    "        # process all results\n",
    "        for future in as_completed(futures):\n",
    "            # open the file and load the data\n",
    "            num_pairs = future.result()\n",
    "            total_pairs += num_pairs\n",
    "    print('Done', total_pairs)\n",
    "\n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    main(chunk_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e99ad835aca01fce0a897b791c0c10d551df5b71b25dd81bca280ed11e20ed7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
